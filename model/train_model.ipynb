{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afabac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnan, lit\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, NGram\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070878eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReviewSentimentClassifier\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer le niveau de log pour réduire les sorties\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Charger les données (prétraitées en pandas et sauvegardées en CSV)\n",
    "df = spark.read.csv(\"../data/cleaned_reviews.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Vérifier le schéma et les données nulles\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6833559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs nulles\n",
    "print(\"\\nNombre de lignes total:\", df.count())\n",
    "null_counts = df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns])\n",
    "print(\"Valeurs nulles par colonne:\")\n",
    "null_counts.show()\n",
    "\n",
    "# Assurer que label est en format numérique et éliminer toute valeur aberrante\n",
    "df = df.filter((col(\"label\") == 0) | (col(\"label\") == 1) | (col(\"label\") == 2))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "\n",
    "# Remplacer les valeurs nulles dans la colonne \"lemmatized_text\" par une chaîne vide\n",
    "df = df.fillna({'lemmatized_text': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher des statistiques sur les classes\n",
    "print(\"\\nDistribution des classes:\")\n",
    "class_counts = df.groupBy(\"label\").count().orderBy(\"label\")\n",
    "class_counts.show()\n",
    "\n",
    "# Calculer les poids pour équilibrer les classes\n",
    "total = df.count()\n",
    "class_weights = class_counts.collect()\n",
    "weights_dict = {row[\"label\"]: total/row[\"count\"] for row in class_weights}\n",
    "print(\"\\nPoids par classe:\")\n",
    "for label, weight in weights_dict.items():\n",
    "    print(f\"Classe {label}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une colonne de poids pour l'algorithme de classification\n",
    "df = df.withColumn(\"weight\", \n",
    "    when(col(\"label\") == 0.0, lit(weights_dict[0.0]))\n",
    "    .when(col(\"label\") == 1.0, lit(weights_dict[1.0]))\n",
    "    .when(col(\"label\") == 2.0, lit(weights_dict[2.0]))\n",
    "    .otherwise(lit(1.0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les données en 80/10/10 (train/validation/test)\n",
    "# Premier split: 80% train, 20% temp\n",
    "train_df, temp_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "# Deuxième split: diviser les 20% restants en deux parts égales (validation/test)\n",
    "validation_df, test_df = temp_df.randomSplit([0.5, 0.5], seed=42)\n",
    "\n",
    "print(f\"\\nDonnées d'entraînement: {train_df.count()} lignes\")\n",
    "print(f\"Données de validation: {validation_df.count()} lignes\")\n",
    "print(f\"Données de test: {test_df.count()} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9122ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation\n",
    "tokenizer = Tokenizer(inputCol=\"lemmatized_text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter extraction de bi-grammes pour capturer les phrases\n",
    "bigram = NGram(n=2, inputCol=\"filtered_words\", outputCol=\"bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fcb3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF avec plus de features\n",
    "hashingTF = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexation de la classe avec gestion des valeurs nulles\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"label\", \n",
    "    outputCol=\"indexedLabel\", \n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Classifieur avec paramètres optimisés et utilisation des poids\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"indexedLabel\", \n",
    "    weightCol=\"weight\",\n",
    "    maxIter=20,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.5\n",
    ")\n",
    "\n",
    "# Pipeline complète\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, bigram, hashingTF, idf, label_indexer, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Entraîner le modèle sur les données d'entraînement\n",
    "    print(\"\\nEntraînement du modèle en cours...\")\n",
    "    model = pipeline.fit(train_df)\n",
    "    \n",
    "    # Évaluer sur les données de validation\n",
    "    print(\"Évaluation du modèle sur l'ensemble de validation...\")\n",
    "    val_predictions = model.transform(validation_df)\n",
    "    \n",
    "    # Calculer les métriques d'évaluation sur la validation\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"indexedLabel\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"f1\"\n",
    "    )\n",
    "    val_f1 = evaluator.evaluate(val_predictions)\n",
    "    \n",
    "    evaluator.setMetricName(\"accuracy\")\n",
    "    val_accuracy = evaluator.evaluate(val_predictions)\n",
    "    \n",
    "    print(f\"\\nRésultats de validation:\")\n",
    "    print(f\"F1-score (validation): {val_f1:.4f}\")\n",
    "    print(f\"Précision (validation): {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Matrice de confusion sur la validation\n",
    "    print(\"\\nMatrice de confusion (validation):\")\n",
    "    val_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "    \n",
    "    # Évaluation finale sur l'ensemble de test\n",
    "    print(\"\\nÉvaluation finale sur l'ensemble de test...\")\n",
    "    test_predictions = model.transform(test_df)\n",
    "    \n",
    "    # Afficher quelques prédictions\n",
    "    print(\"\\nExemples de prédictions (test):\")\n",
    "    test_predictions.select(\"lemmatized_text\", \"label\", \"prediction\", \"probability\").show(5, truncate=30)\n",
    "    \n",
    "    # Afficher la distribution des prédictions sur l'ensemble de test\n",
    "    print(\"\\nDistribution des prédictions (test):\")\n",
    "    test_predictions.groupBy(\"prediction\").count().orderBy(\"prediction\").show()\n",
    "    \n",
    "    # Matrice de confusion simplifiée sur le test\n",
    "    print(\"\\nMatrice de confusion (test):\")\n",
    "    test_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "    \n",
    "    # Calculer les métriques finales sur l'ensemble de test\n",
    "    test_f1 = evaluator.setMetricName(\"f1\").evaluate(test_predictions)\n",
    "    test_accuracy = evaluator.setMetricName(\"accuracy\").evaluate(test_predictions)\n",
    "    test_recall = evaluator.setMetricName(\"weightedRecall\").evaluate(test_predictions)\n",
    "    \n",
    "    print(f\"\\nRésultats d'évaluation finaux (test):\")\n",
    "    print(f\"F1-score: {test_f1:.4f}\")\n",
    "    print(f\"Précision: {test_accuracy:.4f}\")\n",
    "    print(f\"Recall pondéré: {test_recall:.4f}\")\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    model_path = \"../model/balanced_sentiment_model\"\n",
    "    model.write().overwrite().save(model_path)\n",
    "    print(f\"\\nModèle sauvegardé avec succès à: {model_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nErreur pendant l'entraînement: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
